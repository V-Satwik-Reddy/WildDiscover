# ğŸŒ¿ WildDiscover ğŸ«

WildDiscover is a React Native application designed to help users identify flora, fauna, and landmarks using advanced image recognition APIs and on-device machine learning models. Whether you're exploring nature or visiting famous landmarks, WildDiscover provides detailed information about the objects you capture or upload â€” all in a clean, intuitive interface.

---

## ğŸ“œ Features

- **Flora Identification**: Identify plants and get details like scientific name, family, habitat, and uses.
- **Fauna Identification**: Recognize animals, birds, and insects with information about their diet, habitat, and conservation status.
- **Landmark Identification**: Discover famous landmarks and learn about their history, architecture, and significance.
- **Offline Mode (AI Model)**: Identify flora and fauna without internet using a lightweight MobileNet model.
- **History Feature**: Automatically stores identified images to AWS S3 with user-tagged labels and timestamps, retrievable for future reference.
- **Wikipedia Integration**: Provides links and summaries from Wikipedia for additional information.
- **User-Friendly Interface**: Intuitive design for easy navigation and interaction.

---

## ğŸ›  Installation

Follow these steps to set up and run the project locally:

### Prerequisites

1. **Node.js**: Install [Node.js](https://nodejs.org/) (LTS version recommended).
2. **Expo CLI**: Install Expo CLI globally using:

   ```bash
   npm install -g expo-cli
   ```

3. **Android/iOS Emulator or Physical Device**: Ensure you have an emulator set up or a physical device with the Expo Go app installed.

### Steps

1. **Clone the Repository**:

   ```bash
   git clone <repository-url>
   cd WildDiscover
   ```

2. **Install Dependencies**:
   Run the following command to install all required dependencies:

   ```bash
   npm install
   ```

3. **Set Up API Keys**:
   Create a `config/config.js` file and add your API keys:

   ```javascript
   export const GOOGLE_VISION_API_KEY = 'your-google-vision-api-key';
   export const PLANT_NET_API_KEY = 'your-plant-net-api-key';
   export const ANIMAL_API_KEY = 'your-animal-api-key';
   export const ACCESS_KEY_ID = 'your-aws-access-key';
   export const SECRET_ACCESS_KEY = 'your-aws-secret-key';
   export const BUCKET_NAME = 'your-s3-bucket-name';
   ```

4. **Run the Project**:
   Start the development server using:

   ```bash
   npx expo start
   ```

   - Use the Expo Go app to scan the QR code and run the app on your device.
   - Alternatively, use an emulator for Android or iOS.

---

## ğŸ“‚ Project Structure

```plaintext
WildDiscover/
â”œâ”€â”€ api/                   # API integration files
â”œâ”€â”€ components/            # Reusable UI components
â”œâ”€â”€ screens/               # App screens (Home, Flora, Fauna, Landmark, Result, Offline Variants)
â”œâ”€â”€ assets/                # Static assets (icons, images, offline model files)
â”œâ”€â”€ context/               # Context providers (AppMode)
â”œâ”€â”€ config/                # Configuration files (ignored in .gitignore)
â”œâ”€â”€ App.js                 # Main app entry point
â”œâ”€â”€ tfSetup.js             # TensorFlow model initializer
â”œâ”€â”€ predictor.js           # Offline classifier using MobileNet
â”œâ”€â”€ Upload.js              # AWS S3 upload logic
â”œâ”€â”€ app.json               # Expo configuration
â”œâ”€â”€ package.json           # Project dependencies and scripts
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ“¦ Required Libraries

These will be installed automatically with `npm install`.

### Core Dependencies

- `react`, `react-native`, `expo`
- `expo-image-picker`, `expo-file-system`, `expo-status-bar`, `expo-font`
- `@react-navigation/native`, `@react-navigation/stack`
- `@expo-google-fonts/poppins`, `react-native-vector-icons`
- `@tensorflow/tfjs`, `@tensorflow/tfjs-react-native`
- `@react-native-async-storage/async-storage`
- `uuid`, `axios`
- `aws-sdk`, `react-native-aws3`

### Development Dependencies

- `jest`, `axios-mock-adapter`

---

## ğŸš€ Usage

1. **Home Screen**:
   - Choose a category: Flora, Fauna, or Landmark.

2. **Capture or Upload an Image**:
   - Use your camera or select an image from your gallery.

3. **Analyze the Image**:
   - Depending on network availability and mode, the app will use either an online API or the offline MobileNet model.

4. **Auto-Save to History (Optional)**:
   - If enabled, the image will be saved to AWS S3 with metadata.

5. **Explore Further**:
   - Click the Wikipedia link or check the history.

---

## ğŸ“¸ Screenshots

### Home Screen
<img src="./Images/Home.jpg" alt="Home Screen" width="300">

### Flora Identification
<img src="./Images/Flora.jpg" alt="Flora Identification" width="300">

### Fauna Identification
<img src="./Images/Fauna.jpg" alt="Fauna Identification" width="300">

### Landmark Identification
<img src="./Images/Landmark.jpg" alt="Landmark Identification" width="300">

---

## ğŸ§° Technologies Used

- **React Native** with **Expo** for cross-platform mobile development
- **TensorFlow.js (MobileNet)** for on-device image classification
- **AWS S3** for storing identification history
- **Google Vision / PlantNet / API Ninjas** for online identification APIs
- **Wikipedia API** for fetching entity descriptions

---

## ğŸ“ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! If you'd like to contribute, please fork the repository and submit a pull request.

---
